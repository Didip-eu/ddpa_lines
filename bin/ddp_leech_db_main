#!/usr/bin/env python3

"""
Leeches Monasterium's backend xml-database files to construct target file structure system.
"""

# TODO: add "url2path_idx_path":"{root_dir}/url2path_idx.pickle" + code; checking dependencies etc.

# TODO: fix permissions or make choices available in fargv for different file writing permission modes (https://stackoverflow.com/questions/5231901/permission-problems-when-creating-a-dir-with-os-makedirs-in-python)
# TODO: move some functions to ddp_util

# TODO: discuss in group:
# - if we include data types in function documentation, we should do it consistently, right? what is best practice?
# - convention for docstring writing, see https://www.datacamp.com/tutorial/docstrings-python; and include datatypes

# TODO: update docstrings
# TODO: reduce redundancy; move splits into decompose, correct in origin
# TODO: refactor file move
# TODO: add typed option to extend the range of leeching things (e.g., just atomids, images etc.)
# TODO: fix makedirs bug when not creating target files for charters


import os
import random
from pathlib import Path
from pathlib import PurePosixPath
from pprint import pprint
import shutil
from typing import Dict, Generator, List
from lxml import etree
import fargv
from tqdm import tqdm
import hashlib

def get_path_generator(directory: str, file_extension: str) -> Generator:
    """
    Returns Generator of file paths in (sub)directories.
    @param directory: directory of monasterium xml files as a string
    @param file_extension: specifies file type, monasterium files would be .cei.xml
    @return Generator with file paths
    """
    for entry in os.scandir(directory):
        if entry.is_file() and entry.name.endswith(file_extension):
            yield Path(entry.path)
        elif entry.is_dir():
            yield from get_path_generator(entry.path, file_extension)
        else:
            continue


def get_path_list(directory: str, file_extension: str) -> List[str]:
    """
    Returns List containing file paths.
    @param directory: directory of monasterium xml files as a string
    @param file_extension: specifies file type, monasterium files would be .cei.xml
    @return List with file paths
    """
    pprint("Scanning directory for files.")
    paths = [f"{PurePosixPath(path)}" for path in get_path_generator(directory, file_extension)]
    #return random.sample(paths, 50)
    return paths


def get_atomid_dict(paths: List[str]) -> Dict[str, str]:
    """
    Returns unique ids from xml-cei files.
    @param paths: a List containing paths to xml files, which contain an atom id
    @return Dict with path as key and atomd id as value
    """
    pprint("Parsing .cei.xml for atomids.")
    file_locations = {}
    for file in tqdm(paths):
        with open(file, 'r', encoding='utf-8') as current:
            tree = etree.parse(current)
            root = tree.getroot()
            file_locations[file] = root[0].text
    return file_locations


def decompose_chatomid(chatomid):
    """Infers the atom ids of the supercuration (archive/COLLECTIONS) and curation (fond/collection) 
    from a charters atomid
    """
    parts = chatomid.split("/")
    try:
        assert parts[:2] == (['tag:www.monasterium.net,2011:', 'charter'])
    except AssertionError:
        print("atom-id is not well-formed.")
        raise
    if len(parts) == 5:
        supercuration_id = f"{parts[0]}/archive/{parts[2]}"
        curation_id = f"{parts[0]}/fond/{parts[2]}/{parts[3]}"
    elif len(parts) == 4:
        supercuration_id = "COLLECTIONS"
        curation_id = f"{parts[0]}/collection/{parts[2]}"
    return parts, supercuration_id, curation_id


# def chatomid_to_fond_id(chatomid): #maybe add later when refactoring
#     parts = chatomid.split("/")
#     try:
#         assert len(parts) == 5
#     except AssertionError:
#         print("atom-id does not belong to archive fond.")
#         raise
#     try:
#         assert parts[:2] == (['tag:www.monasterium.net,2011:', 'charter'])
#     except AssertionError:
#         print("atom-id is not well-formed.")
#         raise
#     return parts
 

def to_md5(string, trunc_threshold=16): 
    md5sum = hashlib.md5(string.encode('utf-8')).hexdigest()[trunc_threshold:]
    return md5sum


def copy_charter_files(locations: Dict[str, str], target_directory: str): #see decompose_atomid in ddp_util namespace.py
    """
    For Archive Material: Copies folder and their xml contents from input folder to target
    Builds (sub)directory structure at target based on (length of) atomids.
    @param locations: Dict with path as key and atomd id as value
    @param target_directory: string specifying the target directory; where to create folders
    """
    pprint(f"Charters: building (hashed) target directories; copying files.")
    for path, atomid in tqdm(locations.items()):
        parts, supercuration_id, curation_id = decompose_chatomid(atomid)
        curation_hash = to_md5(curation_id) #hash also circumvents output bugs from misconstrued curation names (e.g., double periods; looking at you, CH-StaASG)
        charter_hash = to_md5(atomid)
        if len(parts) == 4:
            target_path = f"{target_directory}/{supercuration_id}/{curation_hash}/{charter_hash}" #change paradigma so it is not confusing; the term _id for supercuration_id is bad here since there is no natural id
            os.makedirs(target_path, exist_ok=True)
            shutil.copy(path, f"{target_path}/{charter_hash}.xml")
        elif len(parts) == 5:
            target_path = f"{target_directory}/{parts[2]}/{curation_hash}/{charter_hash}"
            os.makedirs(target_path, exist_ok=True)
            shutil.copy(path, f"{target_path}/{charter_hash}.CH.cei.xml")
        else:
            raise ValueError("Length of atomid is irregular.")


def copy_collection_files(locations: Dict[str, str], target_directory: str):
    pprint(f"Collections: building (hashed) target directories; copying files.")
    for path, atomid in tqdm(locations.items()):       
        curation_hash = to_md5(atomid)
        target_path = f"{target_directory}/COLLECTIONS/{curation_hash}"
        os.makedirs(target_path, exist_ok=True)
        shutil.copy(path, f"{target_path}/{curation_hash}.CO.cei.xml")


def copy_archive_files(locations: Dict[str, str], target_directory: str):
    pprint(f"Archives: building (hashed) target directories; copying files.")
    for path, atomid in tqdm(locations.items()):
        parts = atomid.split("/") 
        target_path = f"{target_directory}/{parts[2]}"
        os.makedirs(target_path, exist_ok=True)
        shutil.copy(path, f"{target_path}/{parts[2]}.AR.ead.xml")


def copy_fond_files(locations: Dict[str, str], target_directory: str):
    pprint(f"Fonds: building (hashed) target directories; copying files.")
    for path, atomid in tqdm(locations.items()):
        path_pref = f"{Path(path).parent}/{Path(path).with_suffix('').stem}.preferences.xml"     
        try:
            assert Path(path_pref).is_file() #e.g. for AT-VLA/LandesregierungAmt/LandesregierungAmt.preferences.xml'
        except AssertionError:
            print(f"preferences.xml not found for {path}.") #TODO: logging for this, and also logging for general execution of bin
            continue
        
        parts = atomid.split("/") 
        curation_hash = to_md5(atomid)
        target_path = f"{target_directory}/{parts[2]}/{curation_hash}"
        os.makedirs(target_path, exist_ok=True)
        #TODO: Important: account for old.ead.xml 
        #e.g. preferences.xml not found for data/db/mom-data/metadata.fond.public/DE-StAM/SchlossarchivPiesing/SchlossarchivPiesing.old.ead.xml.
        shutil.copy(path, f"{target_path}/{curation_hash}.FO.ead.xml")
        shutil.copy(path_pref, f"{target_path}/{curation_hash}.FO.preferences.xml")

if __name__ == "__main__":
    p = {
        "root_dir": ".",
        #"charter_dir": "{root_dir}/data/db_subsample",
        "charter_dir": "{root_dir}/data/db/mom-data/metadata.charter.public",
        "collection_dir": "{root_dir}/data/db/mom-data/metadata.collection.public",
        "archive_dir": "{root_dir}/data/db/mom-data/metadata.archive.public",
        "fond_dir": "{root_dir}/data/db/mom-data/metadata.fond.public",
        "target_dir": "{root_dir}/data/tmp/data/leech_db"
    }

# params
args, _ = fargv.fargv(p)

# # charters ✓
# charter_paths = get_path_list(args.charter_dir, ".cei.xml")
# atom_id_dict = get_atomid_dict(charter_paths)
# copy_charter_files(atom_id_dict, args.target_dir)

# # collections ✓
collection_paths = get_path_list(args.collection_dir, ".cei.xml")
atom_id_dict = get_atomid_dict(collection_paths)
copy_collection_files(atom_id_dict, args.target_dir)

# # archives ✓
archive_paths = get_path_list(args.archive_dir, ".eag.xml")
atom_id_dict = get_atomid_dict(archive_paths)
copy_archive_files(atom_id_dict, args.target_dir)

# fonds ✓
fond_paths_ead = get_path_list(args.fond_dir, ".ead.xml")
atom_id_dict = get_atomid_dict(fond_paths_ead)
copy_fond_files(atom_id_dict, args.target_dir)