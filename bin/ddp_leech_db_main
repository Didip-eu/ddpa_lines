#!/usr/bin/env python3

"""
Leeches Monasterium's backend xml-database files to construct target file structure system.
Clean target_directory recommended.
"""


# TODO: fix permissions or make choices available in fargv for different file writing permission modes (https://stackoverflow.com/questions/5231901/permission-problems-when-creating-a-dir-with-os-makedirs-in-python)
# TODO: refactor paths to own bin (whole set is only parsed once and paths written into leech_db target)
# optional TODO: refactor file move
# optional TODO: add typed option to extend the range of leeching things (e.g., just atomids, images etc.)
# optional TODO: add "url2path_idx_path":"{root_dir}/url2path_idx.pickle"
# e.g.  ' https://www.monasterium.net/mom/AbbayeDeSaintBertin/0d2cc7d7-161f-425c-8878-1184145bd4b5/charter ': './tmp/data/leech/db/COLLECTIONS/AbbayeDeSaintBertin/f6cea380a393f06fa9fe517ff24877e1',


import os
import random
from pathlib import Path
from pathlib import PurePosixPath
from pprint import pprint
import csv
import shutil
import json
from typing import Dict, Generator, List
from lxml import etree
import fargv
from tqdm import tqdm
import hashlib

from ddp_util import decompose_chatomid, get_path_generator, get_path_list, to_md5, chatomid_to_url, url_to_chatomid, co_atomid_to_url


def get_atomid_dict(paths: List[str]) -> Dict[str, str]:
    """
    Returns unique ids from xml-cei files.
    @param paths: a List containing paths to xml files, which contain an atom id
    @return Dict with path as key and atomd id as value
    """
    pprint("Parsing .cei.xml for atomids.")
    file_locations = {}
    for file in tqdm(paths):
        with open(file, 'r', encoding='utf-8') as current:
            tree = etree.parse(current)
            root = tree.getroot()
            file_locations[root[0].text] = file
    return file_locations


def get_atomids_from_chatomids(chatomids):

    collections = []
    archives = []
    fonds = []

    for chatomid in chatomids:
        parts, supercuration_id, curation_id = decompose_chatomid(chatomid)
        if len(parts) == 4:
            collections.append(curation_id)
        elif len(parts) == 5:
            archives.append(supercuration_id)
            fonds.append(curation_id)
        
    collection_atomids = sorted(list(set(collections)))
    archive_atomids = sorted(list(set(archives)))
    fond_atomids = sorted(list(set(fonds)))
    return collection_atomids, archive_atomids, fond_atomids


def get_charter_db_mapping(locations: Dict[str, str], target_directory: str):
    file_mappings = {}
    for atomid, path in locations.items():
        parts, supercuration_id, curation_id = decompose_chatomid(atomid)
        curation_hash = to_md5(curation_id) # hash circumvents output bugs from misconstrued curation names (e.g., double periods; looking at you, CH-StaASG)
        charter_hash = to_md5(atomid)        
        if len(parts) == 4:
            target_path = f"{target_directory}/{supercuration_id}/{curation_hash}/{charter_hash}" #TODO: change paradigma so it is not confusing; the term _id for supercuration_id is bad here since there is no natural id
        elif len(parts) == 5:
            target_path = f"{target_directory}/{parts[2]}/{curation_hash}/{charter_hash}"
        else:
            raise ValueError("Length of atomid is irregular.")
        file_mappings[atomid] = [path, target_path]
    return file_mappings
  

def save_file_mappings(target_directory: str, filename: str, dictionary): 
    if dictionary:
        with open(f"{Path(target_directory)}/{filename}.json", mode="w") as current:
            json.dump(dictionary, current)


def copy_charters_and_save_paths(file_mappings, atomids=None, amount=100):
    file_paths = {}
    if amount != 100:
        file_mappings = dict(random.sample(file_mappings.items(), int(round(len(file_mappings) / 100 * amount))))
    for atom_id, paths in file_mappings.items():
        if atomids is None or atom_id in atomids:
            db_path, target_path = paths
            Path(target_path).mkdir(parents=True, exist_ok=True)
            shutil.copy(db_path, f"{target_path}/.CH.cei.xml")
            with open(f"{target_path}/atom_id.txt", "w", encoding="utf-8") as f:
                f.write(atom_id) 
            with open(f"{target_path}/url.txt", "w", encoding="utf-8") as f:
                f.write(chatomid_to_url(atom_id))
            file_paths[atom_id] = target_path
    return file_paths


#TODO: fix bug: no collection fsdb json created, and if, erroneously
def get_collection_db_mapping(locations, target_directory):
    file_mappings = {}
    for atomid, path in locations.items():
        curation_hash = to_md5(atomid)
        target_path = f"{target_directory}/COLLECTIONS/{curation_hash}"
        file_mappings[atomid] = [path, target_path]
    return file_mappings


def copy_collections_and_save_paths(file_mappings, atomids=None, amount=100):
    file_paths = {}
    if amount != 100:
        file_mappings = dict(random.sample(file_mappings.items(), int(round(len(file_mappings) / 100 * amount))))
    for atom_id, paths in file_mappings.items():        
        if atomids is None or atom_id in atomids:
            db_path, target_path = paths
            Path(target_path).mkdir(parents=True, exist_ok=True)
            shutil.copy(db_path, f"{target_path}/.CO.cei.xml")
            with open(f"{target_path}/atom_id.txt", "w", encoding="utf-8") as f:
                f.write(atom_id) 
            with open(f"{target_path}/url.txt", "w", encoding="utf-8") as f:
                f.write(co_atomid_to_url(atom_id))
            file_paths[atom_id] = target_path
    return file_paths





# def copy_archive_files(locations: Dict[str, str], target_directory: str):
#     pprint(f"Archives: building (hashed) target directories; copying files.")
#     for atomid, path in tqdm(locations.items()):
#         parts = atomid.split("/") 
#         target_path = f"{target_directory}/{parts[2]}"
#         Path(target_path).mkdir(parents=True, exist_ok=True)
#         shutil.copy(path, f"{target_path}/.AR.ead.xml")


# def copy_fond_files(locations: Dict[str, str], target_directory: str):
#     pprint(f"Fonds: building (hashed) target directories; copying files.")
#     for atomid, path in tqdm(locations.items()):
#         path_pref = f"{Path(path).parent}/{Path(path).with_suffix('').stem}.preferences.xml"     
#         try:
#             assert Path(path_pref).is_file() #e.g. looking at you, AT-VLA/LandesregierungAmt/LandesregierungAmt.preferences.xml'
#         except AssertionError:
#             print(f"preferences.xml not found for {path}.") #TODO: real logging for this, and also logging for general execution of bin, idx
#             continue
#         parts = atomid.split("/") 
#         curation_hash = to_md5(atomid)
#         target_path = f"{target_directory}/{parts[2]}/{curation_hash}"
#         Path(target_path).mkdir(parents=True, exist_ok=True)
#         #TODO: maybe account for old.ead.xml which is now ignored
#         #e.g. preferences.xml not found for data/db/mom-data/metadata.fond.public/DE-StAM/SchlossarchivPiesing/SchlossarchivPiesing.old.ead.xml.
#         shutil.copy(path, f"{target_path}/.FO.ead.xml")
#         shutil.copy(path_pref, f"{target_path}/.FO.preferences.xml")


if __name__ == "__main__":
    p = {
        "root_dir": ".",
        "charter_dir": "{root_dir}/data/db/mom-data/metadata.charter.public/Laureshamensis",
        #"charter_dir": "{root_dir}/data/db/mom-data/metadata.charter.public/AT-ADG",
        #"charter_dir": "{root_dir}/data/db/mom-data/metadata.charter.public",
        "collection_dir": "{root_dir}/data/db/mom-data/metadata.collection.public",
        "archive_dir": "{root_dir}/data/db/mom-data/metadata.archive.public",
        "fond_dir": "{root_dir}/data/db/mom-data/metadata.fond.public",
        "target_dir": "{root_dir}/data/leech_db"
    }

    # params
    args, _ = fargv.fargv(p)

    # directories
    Path(args.target_dir).mkdir(parents=True, exist_ok=True)

    # files
    with open(f"{args.root_dir}/misc/1000_CVCharters/1kcv_urls.csv") as file:
        data = list(csv.reader(file, delimiter=","))
        ch_atomid_list = [url_to_chatomid(i[-1]) for i in data]
    

    # charters
    ## save paths
    charter_paths = get_path_list(args.charter_dir, ".cei.xml", amount=100)
    atom_id_dict = get_atomid_dict(charter_paths)
    charter_file_mappings = get_charter_db_mapping(atom_id_dict, args.target_dir)
    save_file_mappings(args.target_dir, "charter_atomid_2_paths", charter_file_mappings)
    
    ## copy
    ch_file_paths = copy_charters_and_save_paths(charter_file_mappings, atomids=ch_atomid_list)
    save_file_mappings(args.target_dir, "charter_atomid_2_fsdb", ch_file_paths)
    co_atomid_list, ar_atomid_list, fo_atomid_list = get_atomids_from_chatomids(list(ch_file_paths.keys()))
    print(ar_atomid_list)
    print(fo_atomid_list)
    print(co_atomid_list)

    # collections
    ## save paths
    collection_paths = get_path_list(args.collection_dir, ".cei.xml")
    atom_id_dict = get_atomid_dict(collection_paths)
    collection_file_mappings = get_collection_db_mapping(atom_id_dict, args.target_dir)
    save_file_mappings(args.target_dir, "collection_atomid_2_paths", collection_file_mappings)
    
    ## copy
    co_file_paths = copy_collections_and_save_paths(collection_file_mappings, atomids=co_atomid_list)
    save_file_mappings(args.target_dir, "collection_atomid_2_fsdb", co_file_paths)

    # # collections
    # save_pathdict(args.target_dir, "collection_atomid_2_db_path", atom_id_dict)
    # copy_collection_files(atom_id_dict, args.target_dir)

    # # archives
    # archive_paths = get_path_list(args.archive_dir, ".eag.xml")
    # atom_id_dict = get_atomid_dict(archive_paths)
    # save_pathdict(args.target_dir, "archive_atomid_2_db_path", atom_id_dict)
    # copy_archive_files(atom_id_dict, args.target_dir)

    # # fonds
    # fond_paths_ead = get_path_list(args.fond_dir, ".ead.xml")
    # atom_id_dict = get_atomid_dict(fond_paths_ead)
    # save_pathdict(args.target_dir, "fond_atomid_2_db_path", atom_id_dict)
    # copy_fond_files(atom_id_dict, args.target_dir)
