#!/usr/bin/env python3

"""
Leeches Monasterium's backend xml-database files to construct target file structure system.
Clean target_directory recommended.
"""


# TODO: fix permissions or make choices available in fargv for different file writing permission modes (https://stackoverflow.com/questions/5231901/permission-problems-when-creating-a-dir-with-os-makedirs-in-python)
# optional TODO: refactor file move
# optional TODO: add typed option to extend the range of leeching things (e.g., just atomids, images etc.)
# optional TODO: add "url2path_idx_path":"{root_dir}/url2path_idx.pickle"
# e.g.  ' https://www.monasterium.net/mom/AbbayeDeSaintBertin/0d2cc7d7-161f-425c-8878-1184145bd4b5/charter ': './tmp/data/leech/db/COLLECTIONS/AbbayeDeSaintBertin/f6cea380a393f06fa9fe517ff24877e1',


import os
import random
from pathlib import Path
from pathlib import PurePosixPath
from pprint import pprint
import shutil
import json
from typing import Dict, Generator, List
from lxml import etree
import fargv
from tqdm import tqdm
import hashlib

from ddp_util import decompose_chatomid, get_path_generator, get_path_list, to_md5, chatomid_to_url


def get_atomid_dict(paths: List[str]) -> Dict[str, str]:
    """
    Returns unique ids from xml-cei files.
    @param paths: a List containing paths to xml files, which contain an atom id
    @return Dict with path as key and atomd id as value
    """
    pprint("Parsing .cei.xml for atomids.")
    file_locations = {}
    for file in tqdm(paths):
        with open(file, 'r', encoding='utf-8') as current:
            tree = etree.parse(current)
            root = tree.getroot()
            file_locations[root[0].text] = file
    return file_locations


def get_charter_copy_map(locations: Dict[str, str], target_directory: str):
    file_mappings = {}
    for atomid, path in locations.items():
        parts, supercuration_id, curation_id = decompose_chatomid(atomid)
        curation_hash = to_md5(curation_id) # hash circumvents output bugs from misconstrued curation names (e.g., double periods; looking at you, CH-StaASG)
        charter_hash = to_md5(atomid)        
        if len(parts) == 4:
            target_path = f"{target_directory}/{supercuration_id}/{curation_hash}/{charter_hash}" #TODO: change paradigma so it is not confusing; the term _id for supercuration_id is bad here since there is no natural id
        elif len(parts) == 5:
            target_path = f"{target_directory}/{parts[2]}/{curation_hash}/{charter_hash}"
        else:
            raise ValueError("Length of atomid is irregular.")
        file_mappings[atomid] = [path, target_path]
    return file_mappings


def save_file_mappings(target_directory: str, filename: str, dictionary): 
    with open(f"{Path(target_directory)}/{filename}.json", mode="w") as current:
        json.dump(dictionary, current)


def copy_charter_files(file_mappings):
    for atom_id, paths in file_mappings.items():
        print(paths)
        db_path = paths[0]
        target_path = paths[1]
        Path(target_path).mkdir(parents=True, exist_ok=True)
        shutil.copy(db_path, f"{target_path}/.CH.cei.xml")
        with open(f"{target_path}/atom_id.txt", "w", encoding="utf-8") as current:
            current.write(atom_id) 
        with open(f"{target_path}/url.txt", "w", encoding="utf-8") as current:
            current.write(chatomid_to_url(atom_id))

# TODO: think of better solution with regards to file endings (right now: the paths.json includes the full file path for the db, but only the charter folder path hfor the fsdb
# -> is this enough?)




# def copy_charter_files(locations: Dict[str, str], target_directory: str):
#     """
#     For Archive Material: Copies folder and their xml contents from input folder to target.
#     Builds (sub)directory structure at target based on (length of) atomids.
#     Creates atom_id.txt and url.txt at charter level.
#     @param locations: Dict with path as key and atomd id as value
#     @param target_directory: string specifying the target directory; where to create folders
#     """
#     pprint(f"Charters: building (hashed) target directories; copying files.")
    
#     for atomid, path in locations.items():
#         parts, supercuration_id, curation_id = decompose_chatomid(atomid)
#         curation_hash = to_md5(curation_id) # hash circumvents output bugs from misconstrued curation names (e.g., double periods; looking at you, CH-StaASG)
#         charter_hash = to_md5(atomid)
        
#         if len(parts) == 4:
#             target_path = f"{target_directory}/{supercuration_id}/{curation_hash}/{charter_hash}" #TODO: change paradigma so it is not confusing; the term _id for supercuration_id is bad here since there is no natural id
#         elif len(parts) == 5:
#             target_path = f"{target_directory}/{parts[2]}/{curation_hash}/{charter_hash}"
#         else:
#             raise ValueError("Length of atomid is irregular.") 
        
#         Path(target_path).mkdir(parents=True, exist_ok=True)
#         shutil.copy(path, f"{target_path}/.CH.cei.xml")

#         with open(f"{target_path}/atom_id.txt", "w", encoding="utf-8") as current:
#             current.write(atomid) 
#         with open(f"{target_path}/url.txt", "w", encoding="utf-8") as current:
#             current.write(chatomid_to_url(atomid))


def copy_collection_files(locations: Dict[str, str], target_directory: str):
    pprint(f"Collections: building (hashed) target directories; copying files.")
    for atomid, path in tqdm(locations.items()):       
        curation_hash = to_md5(atomid)
        target_path = f"{target_directory}/COLLECTIONS/{curation_hash}"
        Path(target_path).mkdir(parents=True, exist_ok=True)
        shutil.copy(path, f"{target_path}/.CO.cei.xml")


def copy_archive_files(locations: Dict[str, str], target_directory: str):
    pprint(f"Archives: building (hashed) target directories; copying files.")
    for atomid, path in tqdm(locations.items()):
        parts = atomid.split("/") 
        target_path = f"{target_directory}/{parts[2]}"
        Path(target_path).mkdir(parents=True, exist_ok=True)
        shutil.copy(path, f"{target_path}/.AR.ead.xml")


def copy_fond_files(locations: Dict[str, str], target_directory: str):
    pprint(f"Fonds: building (hashed) target directories; copying files.")
    for atomid, path in tqdm(locations.items()):
        path_pref = f"{Path(path).parent}/{Path(path).with_suffix('').stem}.preferences.xml"     
        try:
            assert Path(path_pref).is_file() #e.g. looking at you, AT-VLA/LandesregierungAmt/LandesregierungAmt.preferences.xml'
        except AssertionError:
            print(f"preferences.xml not found for {path}.") #TODO: real logging for this, and also logging for general execution of bin, idx
            continue
        parts = atomid.split("/") 
        curation_hash = to_md5(atomid)
        target_path = f"{target_directory}/{parts[2]}/{curation_hash}"
        Path(target_path).mkdir(parents=True, exist_ok=True)
        #TODO: maybe account for old.ead.xml which is now ignored
        #e.g. preferences.xml not found for data/db/mom-data/metadata.fond.public/DE-StAM/SchlossarchivPiesing/SchlossarchivPiesing.old.ead.xml.
        shutil.copy(path, f"{target_path}/.FO.ead.xml")
        shutil.copy(path_pref, f"{target_path}/.FO.preferences.xml")

# def save__pathdict(target_directory: str, filename: str, db_path_dict: Dict[str, str]): 
#     with open(f"{Path(target_directory)}/{filename}.json", mode="w") as current:
#         json.dump(db_path_dict, current)


if __name__ == "__main__":
    p = {
        "root_dir": ".",
        "charter_dir": "{root_dir}/data/db/mom-data/metadata.charter.public",
        "collection_dir": "{root_dir}/data/db/mom-data/metadata.collection.public",
        "archive_dir": "{root_dir}/data/db/mom-data/metadata.archive.public",
        "fond_dir": "{root_dir}/data/db/mom-data/metadata.fond.public",
        "target_dir": "{root_dir}/data/leech_db"
    }

    # params
    args, _ = fargv.fargv(p)

    # directories
    Path(args.target_dir).mkdir(parents=True, exist_ok=True)

    # charters
    charter_paths = get_path_list(args.charter_dir, ".cei.xml", amount=0.025)
    atom_id_dict = get_atomid_dict(charter_paths)
    charter_file_mappings = get_charter_copy_map(atom_id_dict, args.target_dir)
    copy_charter_files(charter_file_mappings)
    save_file_mappings(args.target_dir, "charter_atomid_2_paths", charter_file_mappings)


    #copy_charter_files(atom_id_dict, args.target_dir)
    #save_pathdict(args.target_dir, "charter_atomid_2_fsdb_path", atom_id_dict)

    # collections
    collection_paths = get_path_list(args.collection_dir, ".cei.xml")
    atom_id_dict = get_atomid_dict(collection_paths)
    save_pathdict(args.target_dir, "collection_atomid_2_db_path", atom_id_dict)
    copy_collection_files(atom_id_dict, args.target_dir)

    # archives
    archive_paths = get_path_list(args.archive_dir, ".eag.xml")
    atom_id_dict = get_atomid_dict(archive_paths)
    save_pathdict(args.target_dir, "archive_atomid_2_db_path", atom_id_dict)
    copy_archive_files(atom_id_dict, args.target_dir)

    # fonds
    fond_paths_ead = get_path_list(args.fond_dir, ".ead.xml")
    atom_id_dict = get_atomid_dict(fond_paths_ead)
    save_pathdict(args.target_dir, "fond_atomid_2_db_path", atom_id_dict)
    copy_fond_files(atom_id_dict, args.target_dir)
