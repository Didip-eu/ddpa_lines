#!/usr/bin/env python3

"""
Leeches Monasterium's backend xml-database files to construct target file structure system.
"""

# TODO: add "url2path_idx_path":"{root_dir}/url2path_idx.pickle" + code; checking dependencies etc.

# TODO: fix permissions or make choices available in fargv for different file writing permission modes (https://stackoverflow.com/questions/5231901/permission-problems-when-creating-a-dir-with-os-makedirs-in-python)

# TODO: discuss in group:
# - if we include data types in function documentation, we should do it consistently, right? what is best practice?
# - convention for docstring writing, see https://www.datacamp.com/tutorial/docstrings-python; and include datatypes

# TODO: update docstrings + Types
# TODO: refactor file move
# TODO: add typed option to extend the range of leeching things (e.g., just atomids, images etc.)

import os
import random
from pathlib import Path
from pathlib import PurePosixPath
from pprint import pprint
import shutil
import json
from typing import Dict, Generator, List
from lxml import etree
import fargv
from tqdm import tqdm
import hashlib

from ddp_util import decompose_chatomid, get_path_generator, get_path_list, to_md5


def get_atomid_dict(paths: List[str]) -> Dict[str, str]:
    """
    Returns unique ids from xml-cei files.
    @param paths: a List containing paths to xml files, which contain an atom id
    @return Dict with path as key and atomd id as value
    """
    pprint("Parsing .cei.xml for atomids.")
    file_locations = {}
    for file in tqdm(paths):
        with open(file, 'r', encoding='utf-8') as current:
            tree = etree.parse(current)
            root = tree.getroot()
            file_locations[file] = root[0].text
    return file_locations


def copy_charter_files(locations: Dict[str, str], target_directory: str):
    """
    For Archive Material: Copies folder and their xml contents from input folder to target; creates atom_id.txt.
    Builds (sub)directory structure at target based on (length of) atomids.
    @param locations: Dict with path as key and atomd id as value
    @param target_directory: string specifying the target directory; where to create folders
    """
    pprint(f"Charters: building (hashed) target directories; copying files.")
    for path, atomid in tqdm(locations.items()):
        parts, supercuration_id, curation_id = decompose_chatomid(atomid)
        curation_hash = to_md5(curation_id) #hash also circumvents output bugs from misconstrued curation names (e.g., double periods; looking at you, CH-StaASG)
        charter_hash = to_md5(atomid)
        if len(parts) == 4:
            target_path = f"{target_directory}/{supercuration_id}/{curation_hash}/{charter_hash}" #TODO: change paradigma so it is not confusing; the term _id for supercuration_id is bad here since there is no natural id
        elif len(parts) == 5:
            target_path = f"{target_directory}/{parts[2]}/{curation_hash}/{charter_hash}"
        else:
            raise ValueError("Length of atomid is irregular.") 
        Path(target_path).mkdir(parents=True, exist_ok=True)
        shutil.copy(path, f"{target_path}/{charter_hash}.CH.cei.xml")
        with open(f"{target_path}/atom_id.txt", 'w', encoding="utf-8") as current:
            current.write(atomid) 


def copy_collection_files(locations: Dict[str, str], target_directory: str):
    pprint(f"Collections: building (hashed) target directories; copying files.")
    for path, atomid in tqdm(locations.items()):       
        curation_hash = to_md5(atomid)
        target_path = f"{target_directory}/COLLECTIONS/{curation_hash}"
        Path(target_path).mkdir(parents=True, exist_ok=True)
        shutil.copy(path, f"{target_path}/{curation_hash}.CO.cei.xml")


def copy_archive_files(locations: Dict[str, str], target_directory: str):
    pprint(f"Archives: building (hashed) target directories; copying files.")
    for path, atomid in tqdm(locations.items()):
        parts = atomid.split("/") 
        target_path = f"{target_directory}/{parts[2]}"
        Path(target_path).mkdir(parents=True, exist_ok=True)
        shutil.copy(path, f"{target_path}/{parts[2]}.AR.ead.xml")


def copy_fond_files(locations: Dict[str, str], target_directory: str):
    pprint(f"Fonds: building (hashed) target directories; copying files.")
    for path, atomid in tqdm(locations.items()):
        path_pref = f"{Path(path).parent}/{Path(path).with_suffix('').stem}.preferences.xml"     
        try:
            assert Path(path_pref).is_file() #e.g. looking at you, AT-VLA/LandesregierungAmt/LandesregierungAmt.preferences.xml'
        except AssertionError:
            print(f"preferences.xml not found for {path}.") #TODO: real logging for this, and also logging for general execution of bin, idx
            continue
        parts = atomid.split("/") 
        curation_hash = to_md5(atomid)
        target_path = f"{target_directory}/{parts[2]}/{curation_hash}"
        Path(target_path).mkdir(parents=True, exist_ok=True)
        #TODO: maybe account for old.ead.xml; discuss this (transfer, but disregard moving to clean production fsdb?); now: ignore
        #e.g. preferences.xml not found for data/db/mom-data/metadata.fond.public/DE-StAM/SchlossarchivPiesing/SchlossarchivPiesing.old.ead.xml.
        shutil.copy(path, f"{target_path}/{curation_hash}.FO.ead.xml")
        shutil.copy(path_pref, f"{target_path}/{curation_hash}.FO.preferences.xml")


if __name__ == "__main__":
    p = {
        "root_dir": ".",
        "charter_dir": "{root_dir}/data/db/mom-data/metadata.charter.public",
        "collection_dir": "{root_dir}/data/db/mom-data/metadata.collection.public",
        "archive_dir": "{root_dir}/data/db/mom-data/metadata.archive.public",
        "fond_dir": "{root_dir}/data/db/mom-data/metadata.fond.public",
        "target_dir": "{root_dir}/data/tmp/data/leech_db"
    }


# params
args, _ = fargv.fargv(p)
print("\n")

# charters ✓
#charter_paths = get_path_list(args.charter_dir, ".cei.xml", sample=True)
charter_paths = get_path_list(args.charter_dir, ".cei.xml")
atom_id_dict = get_atomid_dict(charter_paths)
#copy_charter_files(atom_id_dict, args.target_dir)
json.dump(atom_id_dict, open( f"{args.target_dir}/chatomid_db_paths.json", 'w' )) #save dict
print("\n")

# # collections ✓
# collection_paths = get_path_list(args.collection_dir, ".cei.xml")
# atom_id_dict = get_atomid_dict(collection_paths)
# copy_collection_files(atom_id_dict, args.target_dir)
# print("\n")

# # archives ✓
# archive_paths = get_path_list(args.archive_dir, ".eag.xml")
# atom_id_dict = get_atomid_dict(archive_paths)
# copy_archive_files(atom_id_dict, args.target_dir)
# print("\n")

# # fonds ✓
# fond_paths_ead = get_path_list(args.fond_dir, ".ead.xml")
# atom_id_dict = get_atomid_dict(fond_paths_ead)
# copy_fond_files(atom_id_dict, args.target_dir)
# print("\n")
