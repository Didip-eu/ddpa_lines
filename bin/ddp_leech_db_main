#!/usr/bin/env python3

"""
Leeches Monasterium's backend xml-database files to construct target file structure system.
Clean target_directory recommended.
"""

# TODO: fix permissions or make choices available in fargv for different file writing permission modes (https://stackoverflow.com/questions/5231901/permission-problems-when-creating-a-dir-with-os-makedirs-in-python)
# optional TODO: add "url2path_idx_path":"{root_dir}/url2path_idx.pickle"
# e.g.  ' https://www.monasterium.net/mom/AbbayeDeSaintBertin/0d2cc7d7-161f-425c-8878-1184145bd4b5/charter ': './tmp/data/leech/db/COLLECTIONS/AbbayeDeSaintBertin/f6cea380a393f06fa9fe517ff24877e1',


import os
import random
from pathlib import Path
from pathlib import PurePosixPath
from pprint import pprint
import csv
import shutil
import json
from typing import Dict, Generator, List
from lxml import etree
import fargv
from tqdm import tqdm
import hashlib
import logging
import logging.config
import yaml

from ddp_util import decompose_chatomid, get_path_generator, get_path_list, to_md5, chatomid_to_url, url_to_chatomid, co_atomid_to_url, ar_atomid_to_url, fo_atomid_to_url


def get_atomid_dict(paths: List[str]) -> Dict[str, str]:
    """
    Returns unique ids from xml-cei files.
    @param paths: a List containing paths to xml files, which contain an atom id
    @return Dict with path as key and atomd id as value
    """
    pprint("Parsing .cei.xml for atomids.")
    file_locations = {}
    for file in tqdm(paths):
        with open(file, 'r', encoding='utf-8') as current:
            tree = etree.parse(current)
            root = tree.getroot()
            file_locations[root[0].text] = file
    return file_locations


def get_atomids_from_chatomids(chatomids):
    collections = []
    archives = []
    fonds = []

    for chatomid in chatomids:
        try:
            parts, supercuration_id, curation_id = decompose_chatomid(chatomid)
        except ValueError as e:
            logger.exception(f"{__name__}: Error processing {chatomid}: {e}")            
            continue
        
        if len(parts) == 4:
            collections.append(curation_id)
        elif len(parts) == 5:
            archives.append(supercuration_id)
            fonds.append(curation_id)
        
    collection_atomids = sorted(list(set(collections)))
    archive_atomids = sorted(list(set(archives)))
    fond_atomids = sorted(list(set(fonds)))
    return collection_atomids, archive_atomids, fond_atomids


def save_file_mappings(target_directory: str, filename: str, dictionary): 
    if dictionary:
        with open(f"{Path(target_directory)}/{filename}.json", mode="w") as current:
            json.dump(dictionary, current)


def get_charter_db_mapping(locations: Dict[str, str], target_directory: str):
    file_mappings = {}
    for atomid, path in locations.items():
        try:
            parts, supercuration_id, curation_id = decompose_chatomid(atomid)
        except ValueError as e:
            logger.exception(f"{__name__}: Error processing {atomid}: {e}")
            continue
        curation_hash = to_md5(curation_id)
        charter_hash = to_md5(atomid)
        if len(parts) == 4:
            #TODO: change paradigma so it is not confusing; the term _id for supercuration_id is bad here since there is no natural id
            target_path = f"{target_directory}/{supercuration_id}/{curation_hash}/{charter_hash}" 
        elif len(parts) == 5:
            target_path = f"{target_directory}/{parts[2]}/{curation_hash}/{charter_hash}"
        file_mappings[atomid] = [path, target_path]
    return file_mappings
  

def copy_charters_and_save_paths(file_mappings, atomids=None, amount=100):
    file_paths = {}
    if amount != 100:
        file_mappings = dict(random.sample(file_mappings.items(), int(round(len(file_mappings) / 100 * amount))))
    for atom_id, paths in file_mappings.items():
        if atomids is None or atom_id in atomids:
            db_path, target_path = paths
            Path(target_path).mkdir(parents=True, exist_ok=True)
            shutil.copy(db_path, f"{target_path}/.CH.cei.xml")
            with open(f"{target_path}/CH.atom_id.txt", "w", encoding="utf-8") as f:
                f.write(atom_id) 
            with open(f"{target_path}/CH.url.txt", "w", encoding="utf-8") as f:
                f.write(chatomid_to_url(atom_id))
            file_paths[atom_id] = target_path
    return file_paths


def get_collection_db_mapping(locations, target_directory):
    file_mappings = {}
    for atomid, path in locations.items():
        curation_hash = to_md5(atomid)
        target_path = f"{target_directory}/COLLECTIONS/{curation_hash}"
        file_mappings[atomid] = [path, target_path]
    return file_mappings


def copy_collections_and_save_paths(file_mappings, atomids=None, amount=100):
    file_paths = {}
    if amount != 100:
        file_mappings = dict(random.sample(file_mappings.items(), int(round(len(file_mappings) / 100 * amount))))
    for atom_id, paths in file_mappings.items():        
        if atomids is None or atom_id in atomids:
            db_path, target_path = paths
            Path(target_path).mkdir(parents=True, exist_ok=True)
            shutil.copy(db_path, f"{target_path}/.CO.cei.xml")
            with open(f"{target_path}/CO.atom_id.txt", "w", encoding="utf-8") as f:
                f.write(atom_id) 
            with open(f"{target_path}/CO.url.txt", "w", encoding="utf-8") as f:
                f.write(co_atomid_to_url(atom_id))
            file_paths[atom_id] = target_path
    return file_paths


def get_archive_db_mapping(locations, target_directory):
    file_mappings = {}
    for atomid, path in locations.items():
        parts = atomid.split("/") 
        target_path = f"{target_directory}/{parts[2]}"
        file_mappings[atomid] = [path, target_path]
    return file_mappings


def copy_archives_and_save_paths(file_mappings, atomids=None, amount=100):
    file_paths = {}
    if amount != 100:
        file_mappings = dict(random.sample(file_mappings.items(), int(round(len(file_mappings) / 100 * amount))))
    for atom_id, paths in file_mappings.items():        
        if atomids is None or atom_id in atomids:
            db_path, target_path = paths
            Path(target_path).mkdir(parents=True, exist_ok=True)
            shutil.copy(db_path, f"{target_path}/.AR.eag.xml")
            with open(f"{target_path}/AR.atom_id.txt", "w", encoding="utf-8") as f:
                f.write(atom_id) 
            with open(f"{target_path}/AR.url.txt", "w", encoding="utf-8") as f:
                f.write(ar_atomid_to_url(atom_id))
            file_paths[atom_id] = target_path
    return file_paths


def get_fond_db_mapping(locations, target_directory):
    file_mappings = {}
    for atomid, path in locations.items():
        parts = atomid.split("/")
        curation_hash = to_md5(atomid)
        target_path = f"{target_directory}/{parts[2]}/{curation_hash}"
        file_mappings[atomid] = [path, target_path]
    return file_mappings


def copy_fonds_and_save_paths(file_mappings, atomids=None, amount=100):
    file_paths = {}
    if amount != 100:
        file_mappings = dict(random.sample(file_mappings.items(), int(round(len(file_mappings) / 100 * amount))))
    for atom_id, paths in file_mappings.items():        
        if atomids is None or atom_id in atomids:
            db_path, target_path = paths
            path_pref = f"{Path(db_path).parent}/{Path(db_path).with_suffix('').stem}.preferences.xml"
            Path(target_path).mkdir(parents=True, exist_ok=True)
            shutil.copy(db_path, f"{target_path}/.FO.ead.xml")
            try:
                assert Path(path_pref).is_file()
            except AssertionError as e:
                logger.exception(f"preferences.xml not found for {db_path}: {e}")
            else:
                shutil.copy(path_pref, f"{target_path}/.FO.preferences.xml")
            with open(f"{target_path}/FO.atom_id.txt", "w", encoding="utf-8") as f:
                f.write(atom_id) 
            with open(f"{target_path}/FO.url.txt", "w", encoding="utf-8") as f:
                f.write(fo_atomid_to_url(atom_id))
            file_paths[atom_id] = target_path
    return file_paths


if __name__ == "__main__":
    p = {
        "root_dir": ".",
        "logging_dir": "{root_dir}/logging",
        "charter_dir": "{root_dir}/data/db/mom-data/metadata.charter.public",
        "collection_dir": "{root_dir}/data/db/mom-data/metadata.collection.public",
        "archive_dir": "{root_dir}/data/db/mom-data/metadata.archive.public",
        "fond_dir": "{root_dir}/data/db/mom-data/metadata.fond.public",
        "target_dir": "{root_dir}/data/leech_db",
        "amount": 100,
    }

    # params
    args, _ = fargv.fargv(p)

    # logging
    with open(f"{args.logging_dir}/config/logging.yaml", "r") as stream:
        config = yaml.load(stream, Loader=yaml.FullLoader)

    logging.config.dictConfig(config)
    logger = logging.getLogger("ddp_leech_db_main")
    logger.propagate = False

    # directories
    Path(args.target_dir).mkdir(parents=True, exist_ok=True)

    # charters
    charter_paths = get_path_list(args.charter_dir, ".cei.xml", amount=args.amount)
    atom_id_dict = get_atomid_dict(charter_paths)
    charter_file_mappings = get_charter_db_mapping(atom_id_dict, args.target_dir)
    save_file_mappings(args.target_dir, "charter_atomids_2_paths", charter_file_mappings)
    ch_file_paths = copy_charters_and_save_paths(charter_file_mappings)
    save_file_mappings(args.target_dir, "charter_atomids_2_fsdb", ch_file_paths)
    co_atomid_list, ar_atomid_list, fo_atomid_list = get_atomids_from_chatomids(list(ch_file_paths.keys()))
    
    # collections
    collection_paths = get_path_list(args.collection_dir, ".cei.xml")
    atom_id_dict = get_atomid_dict(collection_paths)
    collection_file_mappings = get_collection_db_mapping(atom_id_dict, args.target_dir)
    save_file_mappings(args.target_dir, "collection_atomids_2_paths", collection_file_mappings)

    co_file_paths = copy_collections_and_save_paths(collection_file_mappings, atomids=co_atomid_list)
    save_file_mappings(args.target_dir, "collection_atomids_2_fsdb", co_file_paths)
    
    # archives
    archive_paths = get_path_list(args.archive_dir, ".eag.xml")
    atom_id_dict = get_atomid_dict(archive_paths)
    archive_file_mappings = get_archive_db_mapping(atom_id_dict, args.target_dir)
    save_file_mappings(args.target_dir, "archive_atomids_2_paths", archive_file_mappings)

    ar_file_paths = copy_archives_and_save_paths(archive_file_mappings, atomids=ar_atomid_list)
    save_file_mappings(args.target_dir, "archive_atomids_2_fsdb", ar_file_paths)
    
    # fonds
    fond_paths = get_path_list(args.fond_dir, "ead.xml")
    atom_id_dict = get_atomid_dict(fond_paths)
    fond_file_mappings = get_fond_db_mapping(atom_id_dict, args.target_dir)
    save_file_mappings(args.target_dir, "fond_atomids_2_paths", fond_file_mappings)
    fo_file_paths = copy_fonds_and_save_paths(fond_file_mappings, atomids=fo_atomid_list)
    save_file_mappings(args.target_dir, "fond_atomids_2_fsdb", fo_file_paths)
    
    
    
"""For FSDB leech comparison"""

    # 1k cv charters
"""#TODO: rewrite so it more dynamically takes in a python list or any list? (and no code rewrite is necessary)
with open(f"{args.root_dir}/misc/1000_CVCharters/1kcv_urls.csv") as file:
    data = list(csv.reader(file, delimiter=","))
    ch_atomid_list = [url_to_chatomid(i[-1]) for i in data]

#charters
charter_paths = get_path_list(args.charter_dir, ".cei.xml", amount=100)
atom_id_dict = get_atomid_dict(charter_paths)
charter_file_mappings = get_charter_db_mapping(atom_id_dict, args.target_dir)
save_file_mappings(args.target_dir, "charter_atomids_2_paths", charter_file_mappings)

ch_file_paths = copy_charters_and_save_paths(charter_file_mappings, atomids=ch_atomid_list)
save_file_mappings(args.target_dir, "charter_atomids_2_fsdb", ch_file_paths)

#sample
with open(f"{args.target_dir}/charter_atomids_2_fsdb.json") as jf:
    ch_file_paths = json.load(jf)

co_atomid_list, ar_atomid_list, fo_atomid_list = get_atomids_from_chatomids(list(ch_file_paths.keys()))

# collections
collection_paths = get_path_list(args.collection_dir, ".cei.xml")
atom_id_dict = get_atomid_dict(collection_paths)
collection_file_mappings = get_collection_db_mapping(atom_id_dict, args.target_dir)
save_file_mappings(args.target_dir, "collection_atomids_2_paths", collection_file_mappings)

co_file_paths = copy_collections_and_save_paths(collection_file_mappings, atomids=co_atomid_list)
save_file_mappings(args.target_dir, "collection_atomids_2_fsdb", co_file_paths)

# archives
archive_paths = get_path_list(args.archive_dir, ".eag.xml")
atom_id_dict = get_atomid_dict(archive_paths)
archive_file_mappings = get_archive_db_mapping(atom_id_dict, args.target_dir)
save_file_mappings(args.target_dir, "archive_atomids_2_paths", archive_file_mappings)

ar_file_paths = copy_archives_and_save_paths(archive_file_mappings, atomids=ar_atomid_list)
save_file_mappings(args.target_dir, "archive_atomids_2_fsdb", ar_file_paths)

# fonds
fond_paths = get_path_list(args.fond_dir, "ead.xml")
atom_id_dict = get_atomid_dict(fond_paths)
fond_file_mappings = get_fond_db_mapping(atom_id_dict, args.target_dir)
save_file_mappings(args.target_dir, "fond_atomids_2_paths", fond_file_mappings)

fo_file_paths = copy_fonds_and_save_paths(fond_file_mappings, atomids=fo_atomid_list)
save_file_mappings(args.target_dir, "fond_atomids_2_fsdb", fo_file_paths) """
