#!/usr/bin/env python3

"""
Downloads images for charters. 
"""

from pathlib import Path
import json
from typing import List
from lxml import etree
import fargv
import hashlib
from urllib import request
from urllib.error import HTTPError, URLError
import yaml
import logging
import logging.config

from ddp_util import get_path_list, to_md5, clean_img_url, clean_img_url_db, get_extension

#TODO: maybe add e exception for all
#TODO: add descriptive string template for handlers
                   
def get_images(paths: List):
    """Takes a list of file paths and downloads them in place. This implementation builds on each charter folder's .txt-files containing urls.
    Tries multiple url variants of the url, i.e., (cleaned) constructed urls or refs from the original .xml tags.
    Saves image_urls.json at charter level.
    Resumable, checks for md5 at charter level.

    Args:
        paths (List): A List of file paths.
    """
    for path in paths:
        target_path = Path(f"{Path(path).parent}")
        json_file = f"{target_path}/image_urls.json"

        # Load the existing JSON data or create an empty dictionary
        if Path(json_file).exists() and Path(json_file).stat().st_size > 0:
            with open(json_file, "r") as jf:
                json_data = json.load(jf)
        else:
            json_data = {}

        with open(f"{target_path}/CH.atom_id.txt", mode="r") as current:
            ch_atom_id = current.readline()
        with open(f"{target_path}/CH.url.txt", mode="r") as current:
            ch_url = current.readline()
        with open(path, mode="r") as current:
            urls = current.read().splitlines()

        image_ref_path = f"{target_path}/{target_path.stem}.image_refs.txt"

        if Path(image_ref_path).exists():
            with open(image_ref_path, mode="r") as image_ref_file:
                image_refs = image_ref_file.read().splitlines()


        # Loop over the URLs
        for i, url in enumerate(urls):
            url_basic = url
            url_cleaned = clean_img_url_db(url)
            if i < len(image_refs):
                url_ref = image_refs[i]
                url_ref_cleaned = clean_img_url_db(url_ref)
            url_variants = [url_basic, url_cleaned, url_ref, url_ref_cleaned]

            # Initialize flags
            any_variant_successful = False
            variant_tried = False
            image_exists = False
            md5_str = False

            # Loop over the URL variants
            for j, v in enumerate(url_variants):
                variant_tried = True

                if not v.startswith("http://") and not v.startswith("https://"):
                    v = "http://" + v                   
                
                try:
                    ext = get_extension(v)
                except (HTTPError, URLError, ValueError):
                    logger.exception(f"Exception in getting extension of '{v}' (variant {j}) in '{target_path}'. Charter atom_id: '{ch_atom_id}'. Charter url: '{ch_url}'.")
                    continue
                
                try:
                    img_bytes = request.urlopen(v).read()
                except: # Very broad, did not work with (HTTPError, URLError, ValueError):
                    logger.exception(f"Exception in requesting '{v}' (variant {j}) in '{target_path}'. Charter atom_id: '{ch_atom_id}'. Charter url: '{ch_url}'.")
                    continue
                
                try:
                    md5_str = hashlib.md5(img_bytes).hexdigest()
                    #TODO: to_md5 from ddp util implements as such: hashlib.md5(string.encode('utf-8')).hexdigest()[trunc_threshold:] - what does encode("-utf-8") do/what would be the difference without it?
                    image_file_path = Path(f"{target_path}/{md5_str}.{ext}")
                    
                    # Check if the image has already been downloaded
                    if f"{md5_str}.{ext}" in json_data and image_file_path.is_file():
                        image_exists = True
                        break

                    try:
                        with open(image_file_path, "wb") as tf:
                            tf.write(img_bytes)
                    except Exception as e:
                        logger.exception(f"Exception in saving image data for '{v}' (variant {j}) in '{target_path}'. Charter atom_id: '{ch_atom_id}'. Charter url: '{ch_url}'. Error: {e}")
                        continue

                    # Update JSON data and save it to the file
                    json_data[f"{md5_str}.{ext}"] = v
                    with open(json_file, "w") as jf:
                        json.dump(json_data, jf, indent=2)
                    any_variant_successful = True
                    break

                except (HTTPError, URLError, ValueError):
                    logger.exception(f"Exception in downloading '{v}' (variant {j}) in '{target_path}'. \n Charter atom_id: '{ch_atom_id}'. \n Charter url: '{ch_url}'.")
                    continue

            # Log
            if any_variant_successful and variant_tried:
                logger.info(f"Download successful for {v} (variant {j}) into '{target_path}'. \n Charter atom_id: '{ch_atom_id}'. \n Charter url: '{ch_url}'.")
            elif image_exists:
                logger.info(f"Image already exists in '{target_path}'. \n Charter atom_id: '{ch_atom_id}'. \n Charter url: '{ch_url}'")
            elif not any_variant_successful and not image_exists:
                logger.critical(f"No successful image downloads into into '{target_path}'. \n Charter atom_id: '{ch_atom_id}'. \n Charter url: '{ch_url}'.")
                

if __name__ == "__main__":
    p = {
        "root_dir": ".",
        "target_dir": "{root_dir}/data/leech_db/",
        "logging_dir": "{root_dir}/logging"
    }

    # Params
    args, _ = fargv.fargv(p)

    # Logging
    with open(f"{args.logging_dir}/config/logging.yaml", "r") as stream:
        config = yaml.load(stream, Loader=yaml.FullLoader)

    logging.config.dictConfig(config)
    logger = logging.getLogger("ddp_leech_db_images")
    logger.setLevel(logging.DEBUG)

    # Download
    image_url_txt_list = get_path_list(args.target_dir,  ".image_urls.txt")
    get_images(image_url_txt_list)

