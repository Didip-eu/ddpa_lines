#!/usr/bin/env python3

"""
Downloads and logs images for charters.
"""

import os
import random
from pathlib import Path
from pathlib import PurePosixPath
from pprint import pprint
import shutil
from typing import Dict, Generator, List
from lxml import etree
import fargv
from tqdm import tqdm
import hashlib
from urllib import request
from itertools import chain
import random

from ddp_util import get_path_list


def read_lines(files):
    lines = []
    for file in files:            
        with open(file, mode="r") as current:
            result = current.readlines()
            lines.append(result)    
    megalist = list(chain(*lines))
    return megalist

# def store_image(image_url, charter_full_path):
#     for n, img_url in enumerate(image_urls):
#         ext = get_extention(img_url)
#         charter_full_path = 
#         try:
#             img_bytes = urllib.request.urlopen(img_url).read()
#             md5_str = hashlib.md5(img_bytes).hexdigest()
#             open(f"{charter_full_path}/{md5_str}.{ext}", "wb").write(img_bytes)
#             relinked_images_html = relinked_images_html.replace(
#                 img_url, f"{md5_str}.{ext}")
#             imgname2imgurls[f"{md5_str}.{ext}"] = img_url
#         except HTTPError:
#             print(f"charter {url} Failed to download : {img_url}")
#             failed.append(img_url)




# from urllib.parse import urlparse

# from ddp_util import to_md5

#TODO: use store_charter parts of saving url; probably refactor

# def download_images(url_list, target):
#     for url in random.sample(url_list, 5):
#         if not urlparse(url).scheme:
#             url = 'http://' + url
#         result = request.urlretrieve(url, f"{target}/{to_md5(url)}")
#         print(result)
#         print(type(result))


if __name__ == "__main__":
    p = {
        "root_dir": ".",
        #"charter_dir": "{root_dir}/data/db_subsample",
        "charter_dir": "{root_dir}/data/db/mom-data/metadata.charter.public",
        "collection_dir": "{root_dir}/data/db/mom-data/metadata.collection.public",
        "archive_dir": "{root_dir}/data/db/mom-data/metadata.archive.public",
        "fond_dir": "{root_dir}/data/db/mom-data/metadata.fond.public",
        "target_dir": "{root_dir}/data/tmp/data/leech_db"
    }

# params
args, _ = fargv.fargv(p)
print("\n")

# 
image_url_txt_list = get_path_list(args.target_dir,  ".image_urls.txt")
image_url_list = read_lines(image_url_txt_list)
#download_images(image_url_list, args.target_dir)

