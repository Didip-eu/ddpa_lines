#!/usr/bin/env python3

"""
Downloads images for charters. 
"""

#TODO: implement logging

import os
import random
from pathlib import Path
from pathlib import PurePosixPath
from pprint import pprint
import shutil
import json
from typing import Dict, Generator, List
from lxml import etree
import fargv
from tqdm import tqdm
import hashlib
from urllib import request
from urllib.error import HTTPError
from urllib.error import URLError
from itertools import chain
import random
import requests
from urllib.request import urlretrieve
import urllib.parse

import yaml
import logging
import logging.config

from ddp_util import get_path_list, to_md5, clean_img_url, clean_img_url_db, get_extension


def get_images(paths):
    for path in paths:
        target_path = Path(f"{Path(path).parent}")
        json_file = f"{target_path}/image_urls.json"

        # Load the existing JSON data or create an empty dictionary
        if Path(json_file).exists():
            with open(json_file, "r") as jf:
                json_data = json.load(jf)
        else:
            json_data = {}

        with open(f"{Path(path).parent}/CH.atom_id.txt", mode="r") as current:
            ch_atom_id = current.readline()
        with open(f"{Path(path).parent}/CH.url.txt", mode="r") as current:
            ch_url = current.readline()
        with open(path, mode="r") as current:
            urls = current.read().splitlines()

            # Loop over the URLs

            #TODO big: check downloads for skips (actual number right now too low)


            for url in urls:
                url_basic = url
                url_cleaned = clean_img_url_db(url)
                url_variants = [url_basic, url_cleaned]

                # Initialize the flags
                first_variant_successful = False
                variant_tried = False

                # Loop over the URL variants
                for i, v in enumerate(url_variants):
                    if first_variant_successful:
                        logger.info(f"Skipping second variant for {url_basic} since first variant was successful.")
                        break

                    variant_tried = True

                    # Handle the URL variant
                    if not v.startswith("http://") and not v.startswith("https://"):
                        v = "http://" + v  # Add default 'http://' schema if missing                    
                    try:
                        ext = get_extension(v)
                    except (HTTPError, URLError, ValueError):
                        logger.exception(f"Exception in getting extension of '{v}' (variant {i}) in '{target_path}'. Charter atom_id: '{ch_atom_id}'. Charter url: '{ch_url}'.")
                        continue
                    try:
                        img_bytes = request.urlopen(v).read()
                    except: #(HTTPError, URLError, ValueError):
                        logger.exception(f"Exception in requesting '{v}' (variant {i}) in '{target_path}'. Charter atom_id: '{ch_atom_id}'. Charter url: '{ch_url}'.")
                        continue
                    try:
                        md5_str = hashlib.md5(img_bytes).hexdigest()

                        # Check if the image has already been downloaded
                        if f"{md5_str}.{ext}" in json_data:
                            logger.info(f"Image {v} has already been downloaded.")
                            continue

                        with open(f"{target_path}/{md5_str}.{ext}", "wb") as tf:
                            tf.write(img_bytes)

                        # Update JSON data and save it to the file
                        json_data[f"{md5_str}.{ext}"] = v
                        with open(json_file, "w") as jf:
                            json.dump(json_data, jf, indent=2)

                        logger.info(f"Downloaded {v} into '{target_path}'.")
                        first_variant_successful = True
                    except (HTTPError, URLError, ValueError):
                        logger.exception(f"Exception in downloading '{v}' (variant {i}) in '{target_path}'. Charter atom_id: '{ch_atom_id}'. Charter url: '{ch_url}'.")
                        continue

                # Log a critical message if no variant was successful and the image was not already downloaded
                if not first_variant_successful and variant_tried and f"{md5_str}.{ext}" not in json_data:
                    logger.critical(f"No variant of {url_basic} was successful in '{target_path}'. Charter atom_id: '{ch_atom_id}'. Charter url: '{ch_url}'.")






if __name__ == "__main__":
    p = {
        "root_dir": ".",
        "target_dir": "{root_dir}/data/leech_db/",
        "logging_dir": "{root_dir}/logging"
    }

    # params
    args, _ = fargv.fargv(p)

    with open(f"{args.logging_dir}/config/logging.yaml", "r") as stream:
        config = yaml.load(stream, Loader=yaml.FullLoader)

    logging.config.dictConfig(config)
    logger = logging.getLogger("ddp_leech_db_images")
    logger.setLevel(logging.DEBUG)

    # download
    image_url_txt_list = get_path_list(args.target_dir,  ".image_urls.txt")
    get_images(image_url_txt_list)

