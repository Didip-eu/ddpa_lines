#!/usr/bin/env python3

"""
Downloads images for charters. 
"""

#TODO: implement logging

import os
import random
from pathlib import Path
from pathlib import PurePosixPath
from pprint import pprint
import shutil
import json
from typing import Dict, Generator, List
from lxml import etree
import fargv
from tqdm import tqdm
import hashlib
from urllib import request
from urllib.error import HTTPError
from urllib.error import URLError
from itertools import chain
import random
import requests
from urllib.request import urlretrieve
import urllib.parse

import yaml
import logging
import logging.config

from ddp_util import get_path_list, to_md5, clean_img_url, get_extension


def get_images(paths):
    for path in paths:
        target_path = Path(f"{Path(path).parent}")
        json_file = f"{target_path}/image_urls.json"
        
        # Load the existing JSON data or create an empty dictionary
        if Path(json_file).exists():
            with open(json_file, "r") as jf:
                json_data = json.load(jf)
        else:
            json_data = {}

        with open(f"{Path(path).parent}/CH.atom_id.txt", mode="r") as current:
            ch_atom_id = current.readline()
        with open(f"{Path(path).parent}/CH.url.txt", mode="r") as current:
            ch_url = current.readline()
        with open(path, mode="r") as current:
            urls = current.read().splitlines()

            for url in urls:
                img_url = clean_img_url(url)
                """Some charters are still not captured, e.g., 'http://images.monasterium.net/pics/114/K.._MOM-Bilddateien._~Goettweigjpgweb._~Ausgewählte Stücke._~StAG__15150819.jpg' 
                """

                if not img_url.startswith("http://") and not img_url.startswith("https://"):
                    img_url = "http://" + img_url  # Add default 'http://' schema if missing
                
                try:
                    ext = get_extension(img_url)
                except (HTTPError, URLError, ValueError):
                    logger.exception(f"Exception in getting extension of cleaned {img_url}, i.e., uncleaned {url} in {path}. Charter atom_id: '{ch_atom_id}'. Charter url: '{ch_url}'.")
                    continue

                try:
                    img_bytes = request.urlopen(img_url).read()
                    md5_str = hashlib.md5(img_bytes).hexdigest()

                    # Check if the image has already been downloaded
                    if f"{md5_str}.{ext}" in json_data:
                        logger.info(f"Image {img_url} has already been downloaded.")
                        continue

                    with open(f"{target_path}/{md5_str}.{ext}", "wb") as tf:
                        tf.write(img_bytes)

                    # Update JSON data and save it to the file
                    json_data[f"{md5_str}.{ext}"] = img_url
                    with open(json_file, "w") as jf:
                        json.dump(json_data, jf, indent=2)

                    logger.info(f"Downloaded {img_url} into '{target_path}'.")
                except (HTTPError, URLError, ValueError):
                    logger.exception(f"Exception in downloading cleaned {img_url}, i.e., uncleaned '{url}' in '{target_path}'. Charter atom_id: '{ch_atom_id}'. Charter url: '{ch_url}'.")


if __name__ == "__main__":
    p = {
        "root_dir": ".",
        "target_dir": "{root_dir}/data/leech_db/",
        "logging_dir": "{root_dir}/logging"
    }

    # params
    args, _ = fargv.fargv(p)

    with open(f"{args.logging_dir}/config/logging.yaml", "r") as stream:
        config = yaml.load(stream, Loader=yaml.FullLoader)

    logging.config.dictConfig(config)
    logger = logging.getLogger("ddp_leech_db_images")
    logger.setLevel(logging.DEBUG)

    # download
    image_url_txt_list = get_path_list(args.target_dir,  ".image_urls.txt")
    get_images(image_url_txt_list)

