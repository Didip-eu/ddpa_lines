{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Early New High German Reference Corpus transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from pathlib import Path\n",
    "from pathlib import PurePosixPath\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directoryPath = \"../../../../../data/corpora/REF/ReF-v1.0.2/ref-mlu/\"\n",
    "fileExtension = \".xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cei_files(base_dir):\n",
    "    for entry in os.scandir(base_dir):\n",
    "        if entry.is_file() and entry.name.endswith(fileExtension):\n",
    "            yield Path(entry.path)\n",
    "        elif entry.is_dir():\n",
    "            yield from get_cei_files(entry.path)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [f\"{PurePosixPath(path)}\" for path in get_cei_files(directoryPath)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = [\"ids\", \"names\", \"tokens_ascii\", \"tokens_utf\", \"headers\"]\n",
    "ids, names, tokens_ascii, tokens_utf, headers = ([] for i in range(len(lists)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in paths:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        root = etree.parse(f).getroot()\n",
    "        ids.append(root.xpath(\"/text/@id\", smart_strings = False))\n",
    "        names.append(root.xpath(\"/text/cora-header/@name\", smart_strings = False))\n",
    "        tokens_ascii.append(root.xpath(\"/text/token/tok_anno/@ascii\", smart_strings = False))\n",
    "        tokens_utf.append(root.xpath(\"/text/token/tok_anno/@utf\", smart_strings = False))\n",
    "        headers.append(root.xpath(\"/text/header/text()\", smart_strings = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = list(zip(ids, names, tokens_ascii, tokens_utf, headers))\n",
    "contents_full = pd.DataFrame(contents).rename(columns={0: \"ids\", 1: \"names\", 2: \"tokens_ascii\", 3: \"tokens_utf\", 4: \"headers\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_full[\"tokens_ascii_as_string\"] = contents_full[\"tokens_ascii\"].str.join(sep=\" \")\n",
    "contents_full[\"tokens_ascii_as_string\"] = [re.sub(r' . ', '. ', str(x)) for x in contents_full[\"tokens_ascii_as_string\"]]\n",
    "contents_full[\"tokens_utf_as_string\"] = contents_full[\"tokens_utf\"].str.join(sep=\" \")\n",
    "contents_full[\"tokens_utf_as_string\"] = [re.sub(r' . ', '. ', str(x)) for x in contents_full[\"tokens_utf_as_string\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directoryPath = \"../../../../../data/corpora/REF/ReF-v1.0.2/ref-rub/\"\n",
    "fileExtension = \".xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cei_files(base_dir):\n",
    "    for entry in os.scandir(base_dir):\n",
    "        if entry.is_file() and entry.name.endswith(fileExtension):\n",
    "            yield Path(entry.path)\n",
    "        elif entry.is_dir():\n",
    "            yield from get_cei_files(entry.path)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [f\"{PurePosixPath(path)}\" for path in get_cei_files(directoryPath)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = [\"ids\", \"names\", \"tokens_ascii\", \"tokens_utf\", \"headers\"]\n",
    "ids, names, tokens_ascii, tokens_utf, headers = ([] for i in range(len(lists)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in paths:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        root = etree.parse(f).getroot()\n",
    "        ids.append(root.xpath(\"/text/@id\", smart_strings = False))\n",
    "        names.append(root.xpath(\"/text/cora-header/@name\", smart_strings = False))\n",
    "        tokens_ascii.append(root.xpath(\"/text/token/tok_anno/@ascii\", smart_strings = False))\n",
    "        tokens_utf.append(root.xpath(\"/text/token/tok_anno/@utf\", smart_strings = False))\n",
    "        headers.append(root.xpath(\"/text/header/text()\", smart_strings = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = list(zip(ids, names, tokens_ascii, tokens_utf, headers))\n",
    "contents_full = pd.DataFrame(contents).rename(columns={0: \"ids\", 1: \"names\", 2: \"tokens_ascii\", 3: \"tokens_utf\", 4: \"headers\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_full[\"tokens_ascii_as_string\"] = contents_full[\"tokens_ascii\"].str.join(sep=\" \")\n",
    "contents_full[\"tokens_ascii_as_string\"] = [re.sub(r' . ', '. ', str(x)) for x in contents_full[\"tokens_ascii_as_string\"]]\n",
    "contents_full[\"tokens_utf_as_string\"] = contents_full[\"tokens_utf\"].str.join(sep=\" \")\n",
    "contents_full[\"tokens_utf_as_string\"] = [re.sub(r' . ', '. ', str(x)) for x in contents_full[\"tokens_utf_as_string\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directoryPath = \"../../../../../data/corpora/REF/ReF-v1.0.2/ref-up/\" #tigerXML\n",
    "fileExtension = \".xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cei_files(base_dir):\n",
    "    for entry in os.scandir(base_dir):\n",
    "        if entry.is_file() and entry.name.endswith(fileExtension):\n",
    "            yield Path(entry.path)\n",
    "        elif entry.is_dir():\n",
    "            yield from get_cei_files(entry.path)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [f\"{PurePosixPath(path)}\" for path in get_cei_files(directoryPath)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = [\"ids\", \"names\", \"tokens\", \"textTypes\", \"genres\", \"languages\", \"languageTypes\", \"languageRegions\", \"languageAreas\", \"centuries\", \"dates\"]\n",
    "ids, names, tokens, textTypes, genres, languages, languageTypes, languageRegions, languageAreas, centuries, dates = ([] for i in range(len(lists)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in paths:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        root = etree.parse(f).getroot()\n",
    "        ids.append(root.xpath(\"/corpus/@id\", smart_strings = False))\n",
    "        names.append(root.xpath(\"/corpus/head/meta/text/text()\", smart_strings = False))\n",
    "        tokens.append(root.xpath(\"/corpus/body/s//graph/terminals/@word\", smart_strings = False))\n",
    "        textTypes.append(root.xpath(\"/corpus/head/meta/text-type/text()\", smart_strings = False))\n",
    "        genres.append(root.xpath(\"/corpus/head/meta/genre/text()\", smart_strings = False))\n",
    "        languages.append(root.xpath(\"/corpus/head/meta/language/text()\", smart_strings = False))\n",
    "        languageTypes.append(root.xpath(\"/corpus/head/meta/language-type/text()\", smart_strings = False))\n",
    "        languageRegions.append(root.xpath(\"/corpus/head/meta/language-region/text()\", smart_strings = False))\n",
    "        languageAreas.append(root.xpath(\"/corpus/head/meta/language-area/text()\", smart_strings = False))\n",
    "        centuries.append(root.xpath(\"/corpus/head/meta/time/text()\", smart_strings = False))\n",
    "        dates.append(root.xpath(\"/ccorpus/head/meta/date/text()\", smart_strings = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = list(zip(ids, names, tokens_ascii, topics, textTypes, genres, languages, languageTypes, languageRegions, languageAreas, centuries, dates, tokens_utf))\n",
    "contents_full = pd.DataFrame(contents).rename(columns={0: \"ids\", 1: \"names\", 2: \"tokens_ascii\", 3: \"topics\", 4: \"textTypes\", 5: \"genres\", 6: \"languages\", 7:\"languageTypes\", 8: \"languageRegions\", 9: \"languageAreas\", 10: \"centuries\", 11: \"dates\", 12: \"tokens_utf\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_full[\"tokens_ascii_as_string\"] = contents_full[\"tokens_ascii\"].str.join(sep=\" \")\n",
    "contents_full[\"tokens_ascii_as_string\"] = [re.sub(r' . ', '. ', str(x)) for x in contents_full[\"tokens_ascii_as_string\"]]\n",
    "contents_full[\"tokens_utf_as_string\"] = contents_full[\"tokens_utf\"].str.join(sep=\" \")\n",
    "contents_full[\"tokens_utf_as_string\"] = [re.sub(r' . ', '. ', str(x)) for x in contents_full[\"tokens_utf_as_string\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name output according to subcorpus\n",
    "\n",
    "timemarker = datetime.today().strftime('%Y-%m-%d-%H%M')\n",
    "contents_full.to_json(f\"../data/output/REF2PD_{timemarker}.json\")\n",
    "contents_full.to_parquet(f\"../data/output/REF2PD_{timemarker}.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('didip')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1630ff8b1d0ef61a156f83efc8887c6988c096bb09158731ce1d0a82979501ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
