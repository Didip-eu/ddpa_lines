{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from pathlib import Path\n",
    "from pathlib import PurePosixPath\n",
    "from datetime import datetime\n",
    "import os\n",
    "import csv \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from pprint import pprint\n",
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "from ddp_util import chatomid_to_url, url_to_chatomid\n",
    "\n",
    "import glob\n",
    "import ddp_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load datasets, make clean dfs and lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json file generated on the respective date by parsing a Monasterium db dump (\"full20220125-1710\") using /notebooks/XML2PD/XML2PD.ipynyb\n",
    "all_df_full = pd.read_json(\"../../data/in/sampling/charters_2022-09-18-2211.json\")\n",
    "all_df = all_df_full.explode(\"atom_id\")[\"atom_id\"].rename(\"atomid\")\n",
    "print(all_df)\n",
    "all_list = all_df.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 1000 charter project sample, downloaded from GSheets\n",
    "m_cv_df = pd.read_csv(\"../../data/in/sampling/1000charters_gsheet.csv\", encoding=\"utf-8\")\n",
    "m_cv_df[\"atomid\"] = m_cv_df[\"URL \"].apply(lambda x : url_to_chatomid(x))\n",
    "m_cv_df = m_cv_df[\"atomid\"]\n",
    "print(m_cv_df)\n",
    "m_cv_list = m_cv_df.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 1000 charter project sample from leech ?\n",
    "#WSL ~/data/didip/tmp/data/leech/1000_Charters ... once copied, unclear where from\n",
    "m_cv_leech_df = pd.read_fwf(\"../../data/in/sampling/atomid_list.txt\", header=None)\n",
    "m_cv_leech_df = m_cv_leech_df.squeeze().rename(\"atomid\")\n",
    "print(m_cv_leech_df)\n",
    "m_cv_leech_list = m_cv_leech_df.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 1000 charter project sample from other leechdump\n",
    "#atzenhofer@21-PX010:/data/anguelos/monasterium/tmp/data/leech$ \n",
    "m_cv_leech_2nd_df = pd.read_fwf(\"../../data/in/sampling/atomid_list_second.txt\", header=None)\n",
    "m_cv_leech_2nd_df = m_cv_leech_2nd_df.squeeze().rename(\"atomid\")\n",
    "print(m_cv_leech_2nd_df)\n",
    "m_cv_leech_2nd_list = m_cv_leech_2nd_df.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 1000 charter NLP sample, downloaded from Unicloud\n",
    "m_nlp_df = pd.read_parquet(\"../../data/in/sampling/1000charters_nlp.parquet\", engine=\"pyarrow\")\n",
    "m_nlp_df = m_nlp_df.explode(\"atom_id\")[\"atom_id\"].reset_index(drop=True)\n",
    "print(m_nlp_df)\n",
    "m_nlp_list = m_nlp_df.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make list of lists for contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_list = [m_nlp_list, m_cv_list, m_cv_leech_list, m_cv_leech_2nd_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_var_name(variable):\n",
    "    globals_dict = globals()\n",
    "    return [var_name for var_name in globals_dict if globals_dict[var_name] is variable]\n",
    "\n",
    "def isect(list1, list2):\n",
    "    isect = list(set(list1).intersection(set(list2)))\n",
    "    return isect\n",
    "\n",
    "def diff(list1, list2):\n",
    "    diff = list(set(list1).difference(set(list2)))\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = \"\\t\"\n",
    "nl = \"\\n\"\n",
    "sorted_list = sorted(lists_list)\n",
    "\n",
    "for i in sorted_list:\n",
    "    print(f\"{(get_var_name(i))[0]} has a length of {len(i)}\") #question to self: why is get-var-name inconsistent as to the order\n",
    "\n",
    "print(nl)\n",
    "\n",
    "for i in sorted_list:\n",
    "    for j in sorted_list:\n",
    "        intersection = isect(i, j)\n",
    "        difference = diff(i, j)\n",
    "        if i == j:\n",
    "            break\n",
    "        else:\n",
    "          print(f\"{(get_var_name(i))[0]} {tab} and {tab} {(get_var_name(j)[-2])} {tab} -> intersection of {len(intersection)} and a difference of {len(difference)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from co-programming w anguelos: check 1000 charter dirs\n",
    "#print(glob.glob(\"../../misc/1000_Charters/*/*/*/*json\"))\n",
    "# daniel_dirs = sorted(set([\"/\".join(f.split(\"/\")[:-1]) for f in glob.glob(\"../../misc/1000_Charters/*/*/*/*.*.json\")]))\n",
    "# daniel_atoms = [open(f\"{f}/url.txt\").read() for f in daniel_dirs]\n",
    "# daniel_atoms\n",
    "\n",
    "#print(len(daniel_atoms))\n",
    "# print(len(daniel_dirs))\n",
    "# print(set(daniel_dirs)-set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chatomid_to_url(\"tag:www.monasterium.net,2011:/charter/AT-StaAR/UrkDominikanerkloster/StA_Retz%7CD%7CU1%7C1303\"))\n",
    "# print(chatomid_to_url(\"tag:www.monasterium.net,2011:/charter/AT-StaAR/UrkDominikanerkloster/StA_Retz%7CD\"))\n",
    "\n",
    "# \"tag:www.monasterium.net,2011:/charter/SK-SNA/4156-SukromnyArchivBratislavskejKapituly/636\"\n",
    "# \"tag:www.monasterium.net,2011:/charter/SK-SNA/4156-SukromnyArchivBratislavskejKapi\"\n",
    "\n",
    "#80 lengths cutoff?\n",
    "# \"tag:www.monasterium.net,2011:/charter/AT-NOeLA/HA_Seefeld-HardeggerUrk/Hardegger_\"\n",
    "# \"tag:www.monasterium.net,2011:/charter/SK-SNA/4156-SukromnyArchivBratislavskejKapi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check set of sample atomids\n",
    "conclusion: buggy urls -> buggy atomids -> uneven number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_list = m_nlp_list + m_cv_list + m_cv_leech_list + m_cv_leech_2nd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(union_list)\n",
    "len(set(union_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reduce by previous nlp and cv sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_less_nlp = all_df[~all_df.isin(m_nlp_list)]\n",
    "all_less_cv = all_df[~all_df.isin(m_cv_list)]\n",
    "all_less_nlp_cv = all_df[~all_df.isin(m_nlp_list)][~all_df.isin(m_cv_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get charters with tenor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_w_tenor = (all_df_full[all_df_full[\"cei_tenor_joined\"].astype(str) != \"\"])[\"atom_id\"].explode(\"atom_id\")\n",
    "all_w_tenor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deduce previous nlp sample from all tenors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_w_tenor_reduced = all_w_tenor[~all_w_tenor.isin(m_nlp_list)][~all_w_tenor.isin(m_cv_list)]\n",
    "all_w_tenor_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate new 1k nlp sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_nlp_sample = all_w_tenor_reduced.sample(n=1000, random_state=50)\n",
    "new_nlp_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deduce new nlp sample from all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_less_nlp_cv_nlp2 = all_less_nlp_cv[~all_less_nlp_cv.isin(new_nlp_sample)].reset_index(drop=True)\n",
    "all_less_nlp_cv_nlp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate new 3k sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_3k = all_less_nlp_cv_nlp2.sample(n=3000, random_state=50)\n",
    "sample_3k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deduce 3k sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_less_samples = all_less_nlp_cv_nlp2[~all_less_nlp_cv_nlp2.isin(sample_3k)].reset_index(drop=True)\n",
    "all_less_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_df))\n",
    "print(len(all_less_nlp))\n",
    "print(len(all_less_cv))\n",
    "print(len(all_less_nlp_cv))\n",
    "print(len(all_less_nlp_cv_nlp2))\n",
    "print(len(all_less_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extend atomids of 1k charters by 3k charters; +list of new nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomids_3k = sample_3k.to_list()\n",
    "atomids_4k = list(chain(m_cv_list, sample_3k.to_list()))\n",
    "atomids_1k_nlp_new = new_nlp_sample.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_3k = [chatomid_to_url(i) for i in atomids_3k]\n",
    "urls_4k = [chatomid_to_url(i) for i in atomids_4k]\n",
    "urls_1k_nlp_new = [chatomid_to_url(i) for i in atomids_1k_nlp_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(a=50)\n",
    "# for i in random.sample(urls_4k, 10):\n",
    "#     print(i)\n",
    "# random.getstate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../../data/out/samples/urls_3k.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file, delimiter = \"\\n\")\n",
    "    writer.writerow(urls_3k)\n",
    "\n",
    "with open(f\"../../data/out/samples/urls_4k.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file, delimiter = \"\\n\")\n",
    "    writer.writerow(urls_4k)\n",
    "\n",
    "with open(f\"../../data/out/samples/urls_1k_nlp_new.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file, delimiter = \"\\n\")\n",
    "    writer.writerow(urls_1k_nlp_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## atomids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../../data/out/samples/atomids_3k.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file, delimiter = \"\\n\")\n",
    "    writer.writerow(atomids_3k)\n",
    "\n",
    "with open(f\"../../data/out/samples/atomids_4k.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file, delimiter = \"\\n\")\n",
    "    writer.writerow(atomids_4k)\n",
    "\n",
    "with open(f\"../../data/out/samples/atomids_1k_nlp_new.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file, delimiter = \"\\n\")\n",
    "    writer.writerow(atomids_1k_nlp_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HU in samples\n",
    "says:\n",
    "In the cv 1k charters project, the amount of /HU- charters was 38.5%.\n",
    "\n",
    "In the old nlp sample, it was 0.63%.\n",
    "\n",
    "In the new nlp sample, it is 0.6%.\n",
    "\n",
    "In the new 3k one, it is 25.33%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check amount of /HU- in samples\n",
    "n = \"\\n\"\n",
    "exploded = all_df_full.explode(\"atom_id\")\n",
    "df = exploded[exploded[\"atom_id\"].isin(m_cv_list)]\n",
    "m_cv_HU = df[df[\"atom_id\"].astype(str).str.contains(\"/HU-\")]\n",
    "\n",
    "df = exploded[exploded[\"atom_id\"].isin(m_nlp_list)]\n",
    "m_nlp_HU_old = df[df[\"atom_id\"].astype(str).str.contains(\"/HU-\")]\n",
    "\n",
    "df = exploded[exploded[\"atom_id\"].isin(new_nlp_sample.to_list())]\n",
    "m_nlp_HU_new = df[df[\"atom_id\"].astype(str).str.contains(\"/HU-\")]\n",
    "\n",
    "df = exploded[exploded[\"atom_id\"].isin(atomids_3k)]\n",
    "sample_3k_HU = df[df[\"atom_id\"].astype(str).str.contains(\"/HU-\")]\n",
    "\n",
    "# difference of HU % in samples\n",
    "print(f\"\"\"In the cv 1k charters project, the amount of /HU- charters was {round((len(m_cv_HU))/1000*100,2)}%.\n",
    "{n}In the old nlp sample, it was {round((len(m_nlp_HU_old))/3000*100,2)}%.\n",
    "{n}In the new nlp sample, it is {round((len(m_nlp_HU_new))/3000*100,2)}%.\n",
    "{n}In the new 3k one, it is {round((len(sample_3k_HU))/3000*100,2)}%. \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('didipcv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51de90f0d358e009ce9df81fe88f979b0f6e2c5f2b81c54b9ff289164d1b3424"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
